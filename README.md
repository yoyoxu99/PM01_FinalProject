# PM01_FinalProject

ðŸ“Œ Overview
This repository contains the implementation and experimental code for the final course project of Profilierungsmodul Computerlinguistik I â€“ Trustworthy Data-centric AI.
The project focuses on evaluating and analyzing Theory of Mind (ToM) reasoning abilities in Large Language Models (LLMs) using different probing methods.
The main goal of this project is not performance optimization, but to critically examine and improve existing evaluation protocols in the context of trustworthy and data-centric AI.

ðŸ“‚ Project Structure
PM01_FinalProject/
â”‚
â”œâ”€â”€ data/                  # Input datasets (JSONL format)
â”‚   â”œâ”€â”€ False_Belief_Task.jsonl # Original datasets
â”‚   â””â”€â”€ Index4_5_Location.jsonl
â”‚
â”œâ”€â”€ results/               # Model outputs
â”‚   â”œâ”€â”€ mc_*.jsonl
â”‚   â”œâ”€â”€ lm_*.jsonl
â”‚   â””â”€â”€ fr_*.jsonl
â”‚
â”œâ”€â”€ src/                   # Source code
â”‚   â”œâ”€â”€ run_*_model.py     # call model & generate outputs
â”‚   â”œâ”€â”€ eval_*.py          # evaluation
â”‚   â”œâ”€â”€ lm.py              # Prompt Templates + Prompt Builder +  Extract Answer + Aggregation + Scoring
â”‚   â”œâ”€â”€ mc.py
â”‚   â””â”€â”€ fr.py
â”‚
â”œâ”€â”€ logs/                  # evaluation performance (.txt)
â”œâ”€â”€ Figures/               # Figures in the thesis
â”œâ”€â”€ summary_final.csv/     # final evaluation performance, from parsing logs
â”œâ”€â”€ fr_avg.csv             # average accuracy of FR probing, from parsing logs
â””â”€â”€ README.md              # Project documentation

